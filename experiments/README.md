# LLM Benchmark Evaluation Suite

## ğŸ“‹ Project Introduction

This project is a comprehensive Large Language Model (LLM) evaluation suite, containing three benchmarks across different dimensions to comprehensively evaluate the capabilities of large models in Chinese retrieval reasoning, high-difficulty problem solving, and long text understanding.

## ğŸ¯ Evaluation System

This suite includes the following three benchmarks:

### 1ï¸âƒ£ BrowseComp-ZH 

[Detailed Documentation](./BrowseComp-ZH/README.md)

### 2ï¸âƒ£ HLE 

[Detailed Documentation](./HLE/README.md)

### 3ï¸âƒ£ LongBench - Long Text Understanding Evaluation

[Detailed Documentation](./LongBench/README.md)

## ğŸ“Š Evaluation Comparison

| Benchmark | Evaluation Dimension | Data Scale | Core Technology | Key Metrics |
|-----------|---------------------|-----------|-----------------|-------------|
| **BrowseComp-ZH** | Chinese Retrieval Reasoning | 289 questions | Query-focused Summarization<br/>LingoEDU Structural Decomposition | Accuracy, Calibration Error |
| **HLE** | High-Difficulty Problem Solving | Custom | Solver-Selector Architecture<br/>LingoEDU Structural Decomposition | Correctness |
| **LongBench** | Long Text Understanding | Custom | LingoEDU Structural Decomposition<br/>Rerank Semantic Compression | Accuracy, Compression Rate |

## ğŸš€ Quick Start

### Environment Requirements

Each benchmark has independent dependency management:

```bash
# BrowseComp-ZH
cd BrowseComp-ZH
pip install -r requirements.txt

# HLE
cd HLE
pip install -r requirements.txt

# LongBench
cd LongBench
pip install requests tiktoken
```

### Configure API Keys

Each benchmark requires configuration of corresponding API keys:

1. **BrowseComp-ZH**: Configure model API keys in `run.py`
2. **HLE**: Configure model and search API keys in `main.py`
3. **LongBench**: Create `config.py` to configure EDU and Rerank API keys

### Run Evaluation

```bash
# BrowseComp-ZH Standard Evaluation
cd BrowseComp-ZH
bash run.sh

# HLE Full Enhanced Evaluation
cd HLE
python main.py --batch_file hle.json --enable_search --enable_edu

# LongBench Simple Example
cd LongBench
python simple_example.py
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ BrowseComp-ZH/              # Chinese Web Browsing Capability Evaluation
â”‚   â”œâ”€â”€ data/                      # Dataset
â”‚   â”œâ”€â”€ raw_data/                  # Decrypted data
â”‚   â”œâ”€â”€ run.py                     # Standard evaluation
â”‚   â”œâ”€â”€ rag.py                     # Query-focused Summarization
â”‚   â”œâ”€â”€ deepsearch.py              # LingoEDU Structural Decomposition
â”‚   â”œâ”€â”€ edu.py                     # LingoEDU tool
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ HLE/                        # High-Difficulty Problem Solving Evaluation
â”‚   â”œâ”€â”€ llm_agent/                 # Agent core module
â”‚   â”œâ”€â”€ prompt/                    # Prompt templates
â”‚   â”œâ”€â”€ main.py                    # Main program
â”‚   â”œâ”€â”€ hle_score.py               # Scoring script
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ LongBench/                  # Long Text Understanding Evaluation
â”‚   â”œâ”€â”€ edu_rerank_example.py      # Core implementation
â”‚   â”œâ”€â”€ simple_example.py          # Usage example
â”‚   â”œâ”€â”€ config_example.py          # Configuration example
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md                   # This document
```

## ğŸ“ Usage Scenarios

### Scenario 1: Comprehensive Model Capability Evaluation

If you want to comprehensively evaluate a large model's capabilities, you can test on all three benchmarks:

```bash
# 1. Test Chinese retrieval reasoning capability
cd BrowseComp-ZH && bash run.sh

# 2. Test high-difficulty problem solving capability
cd ../HLE && python main.py --batch_file hle.json

# 3. Test long text understanding capability
cd ../LongBench && python simple_example.py
```

### Scenario 2: Specific Capability Evaluation

Choose specific benchmarks based on requirements:

- **Need to evaluate search capability?** â†’ BrowseComp-ZH
- **Need to evaluate reasoning capability?** â†’ HLE
- **Need to evaluate long text processing?** â†’ LongBench

